{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "499cb3c0",
   "metadata": {},
   "source": [
    "### Notice: \n",
    "          You should start the service of MLModel service before all of this. And make sure this notebook runs at content of ML-Model-CI/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517cff98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xingd/PycharmProjects/ML-Model-CI/modelci/hub/client/trt_client.py:22: UserWarning: Module `tensorrtserver` not installed. You are not able to use TRT Client.\n",
      "  warnings.warn('Module `tensorrtserver` not installed. You are not able to use TRT Client.')\n"
     ]
    }
   ],
   "source": [
    "from modelci.persistence.service_ import get_by_id\n",
    "from modelci.hub.client.torch_client import CVTorchClient\n",
    "from modelci.hub.profile_ import Profiler\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6800a9",
   "metadata": {},
   "source": [
    "### 1.Init Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1046bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.rand(3,224,224)\n",
    "# please assign your own torchscipt format model id, you can get one from the example resnet50 torch model by convert cli\n",
    "torchscript_model_id = \"60adf7031ab931b4b8741d49\"\n",
    "model = get_by_id(torchscript_model_id) \n",
    "torch_client = CVTorchClient(img, model, 20, 1)\n",
    "profiler = Profiler(model, torch_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf210df",
   "metadata": {},
   "source": [
    "### 2.Start Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6664fe8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container:mlmodelci_torch_server is already serving.\n",
      " latency: 0.2061 sec throughput: 4.8518 req/sec\n",
      " latency: 0.1933 sec throughput: 5.1734 req/sec\n",
      " latency: 0.2335 sec throughput: 4.2825 req/sec\n",
      " latency: 0.2625 sec throughput: 3.8089 req/sec\n",
      " latency: 0.2092 sec throughput: 4.7800 req/sec\n",
      " latency: 0.1806 sec throughput: 5.5357 req/sec\n",
      " latency: 0.1842 sec throughput: 5.4294 req/sec\n",
      " latency: 0.2122 sec throughput: 4.7134 req/sec\n",
      " latency: 0.2293 sec throughput: 4.3606 req/sec\n",
      " latency: 0.2117 sec throughput: 4.7237 req/sec\n",
      " latency: 0.1858 sec throughput: 5.3835 req/sec\n",
      " latency: 0.2054 sec throughput: 4.8689 req/sec\n",
      " latency: 0.2529 sec throughput: 3.9539 req/sec\n",
      " latency: 0.1998 sec throughput: 5.0053 req/sec\n",
      " latency: 0.2132 sec throughput: 4.6911 req/sec\n",
      " latency: 0.1682 sec throughput: 5.9456 req/sec\n",
      " latency: 0.2637 sec throughput: 3.7922 req/sec\n",
      " latency: 0.1969 sec throughput: 5.0787 req/sec\n",
      " latency: 0.1892 sec throughput: 5.2841 req/sec\n",
      " latency: 0.2172 sec throughput: 4.6037 req/sec\n",
      "\n",
      "\n",
      "testing device: Intel(R) Core(TM) i5-6300HQ CPU @ 2.30GHz\n",
      "total batches: 20, batch_size: 1\n",
      "total latency: 4.222737789154053 s\n",
      "total throughput: 4.7362637697678664 req/sec\n",
      "50th-percentile latency: 0.2076578140258789 s\n",
      "95th-percentile latency: 0.2626039981842041 s\n",
      "99th-percentile latency: 0.26348099708557127 s\n",
      "total GPU memory: 0 bytes\n",
      "average GPU memory usage percentage: 0.0000\n",
      "average GPU memory used: 0 bytes\n",
      "average GPU utilization: 0.0000%\n",
      "completed at 2021-05-26 15:27:42.347283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DynamicProfileResult(device_id='cpu', device_name='Intel(R) Core(TM) i5-6300HQ CPU @ 2.30GHz', batch=1, memory=ProfileMemory(total_memory=0, memory_usage=0, utilization=0.0), latency=ProfileLatency(init_latency=InfoTuple(avg=0.0, p50=0.0, p95=0.0, p99=0.0), preprocess_latency=InfoTuple(avg=0.0, p50=0.0, p95=0.0, p99=0.0), inference_latency=InfoTuple(avg=0.21113688945770265, p50=0.2076578140258789, p95=0.2626039981842041, p99=0.26348099708557127), postprocess_latency=InfoTuple(avg=0.0, p50=0.0, p95=0.0, p99=0.0)), throughput=ProfileThroughput(batch_formation_throughput=0, preprocess_throughput=0, inference_throughput=4.7362637697678664, postprocess_throughput=0), ip=IPv4Address('10.132.25.189'), create_time=datetime.datetime(2021, 5, 26, 15, 27, 42, 347283))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiler.diagnose(server_name=profiler.pre_deploy(device='cpu'), device='cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
